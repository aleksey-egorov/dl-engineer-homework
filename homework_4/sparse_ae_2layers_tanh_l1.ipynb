{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import mnist, plot_graphs, plot_mnist\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "           ])\n",
    "train_loader, valid_loader, test_loader = mnist(valid=10000, transform=mnist_transform, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, latent_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size=10):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 512 )\n",
    "        self.fc3 = nn.Linear(512, 28*28)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, latent_size=10, loss_fn=F.mse_loss, lr=1e-4, l2=0.):\n",
    "        super(Net, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.E = Encoder(latent_size)\n",
    "        self.D = Decoder(latent_size)\n",
    "        self.latent = None\n",
    "        self.loss_fn = loss_fn\n",
    "        self._rho_loss = None\n",
    "        self._loss = 0.    \n",
    "        self.optim = optim.Adam(self.parameters(), lr=lr, weight_decay=l2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        self.latent = self.E(x)\n",
    "        #self.data_rho = h.mean(0)\n",
    "        out = self.D(self.latent)\n",
    "        return out\n",
    "    \n",
    "    def encode(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.E(x)\n",
    "    \n",
    "    def decode(self, h):\n",
    "        with torch.no_grad():\n",
    "            return self.D(h)\n",
    "    \n",
    "    def rho_loss(self, rho, size_average=True):        \n",
    "        self._rho_loss = F.l1_loss(self.latent, torch.eye(self.latent_size))                \n",
    "        return self._rho_loss\n",
    "    \n",
    "    def loss(self, x, target, **kwargs):\n",
    "        target = target.view(-1, 28*28)\n",
    "        self._loss = self.loss_fn(x, target, **kwargs)\n",
    "        return self._loss\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'50': Net(50)}\n",
    "rho = 1/20\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, models, log=None):\n",
    "    train_size = len(train_loader.sampler)\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        for model in models.values():\n",
    "            model.optim.zero_grad()\n",
    "            output = model(data)\n",
    "            rho_loss = model.rho_loss(rho)\n",
    "            loss = model.loss(output, data) + rho_loss\n",
    "            #print (\"rho_loss=\", rho_loss)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 500 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "        if log is not None:\n",
    "            for k in models:\n",
    "                log[k].append((models[k]._loss, models[k]._rho_loss))\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lambda = lambda l: 'loss: {:.4f}'.format(l)\n",
    "rho_lambda = lambda p: 'pho_loss: {:.10f}'.format(p)\n",
    "line = lambda i, l, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + rho_lambda(p)\n",
    "    \n",
    "def test(models, loader, log=None):\n",
    "    test_size = len(loader.sampler)\n",
    "\n",
    "\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    pho_loss = {k: 0. for k in models}\n",
    "    with torch.no_grad():\n",
    "        for data, _ in loader:\n",
    "            output = {k: m(data) for k, m in models.items()}\n",
    "            for k, m in models.items():\n",
    "                test_loss[k] += m.loss(output[k], data, size_average=False).item() # sum up batch loss\n",
    "                pho_loss[k] += m.rho_loss(rho)\n",
    "               #print (\"test_rho_loss=\", m.rho_loss(rho))\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= (test_size * 784)\n",
    "        pho_loss[k] /= (test_size * models[k].latent_size)\n",
    "        if log is not None:\n",
    "            log[k].append((test_loss[k], pho_loss[k]))\n",
    "    \n",
    "    lines = '\\n'.join([line(k, test_loss[k], pho_loss[k]) for k in models]) + '\\n'\n",
    "    report = 'Test set:\\n' + lines        \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksey/.local/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLosses 50: 0.920244\n",
      "Train Epoch: 1 [25000/50000 (50%)]\tLosses 50: 0.248345\n",
      "Train Epoch: 1 [50000/50000 (100%)]\tLosses 50: 0.247805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksey/.local/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "50: loss: 0.2425\tpho_loss: 0.0000109982\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLosses 50: 0.246199\n",
      "Train Epoch: 2 [25000/50000 (50%)]\tLosses 50: 0.247176\n",
      "Train Epoch: 2 [50000/50000 (100%)]\tLosses 50: 0.242837\n",
      "Test set:\n",
      "50: loss: 0.2410\tpho_loss: 0.0000098319\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLosses 50: 0.228164\n",
      "Train Epoch: 3 [25000/50000 (50%)]\tLosses 50: 0.231062\n",
      "Train Epoch: 3 [50000/50000 (100%)]\tLosses 50: 0.261953\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models, train_log)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, valid_loader, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(models['50'], './model_50_tanh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./model_50_tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = next(iter(test_loader))\n",
    "output = model(data)\n",
    "to_plot = output.view(-1, 1, 28, 28).clamp(-1, 1).data.numpy()\n",
    "decoded = model.decode(torch.eye(50))\n",
    "dec_to_plot = decoded.view(-1, 1, 28, 28).clamp(-1, 1).data.numpy()\n",
    "with torch.no_grad():\n",
    "    encoded = model.E(data.view(-1, 28*28))\n",
    "    print (\"encoded=\",encoded)\n",
    "    print((abs(encoded) > 0.02).sum(1))\n",
    "    encoded[abs(encoded) < 0.02] = 0.\n",
    "    decoded_f = model.decode(encoded)\n",
    "    f_to_plot = ((decoded_f.view(-1, 1, 28, 28)+1)*0.5).clamp(-1, 1).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (\"Исходные данные\")\n",
    "plot_mnist(data.data.numpy(), (5, 10))\n",
    "print (\"Выход декодера\")\n",
    "plot_mnist(to_plot, (5, 10))\n",
    "print (\"Выход декодера с отсечкой на латентном слое\")\n",
    "plot_mnist(f_to_plot, (5, 10))\n",
    "print (\"Выход декодера с единичной матрицы\")\n",
    "plot_mnist(dec_to_plot, (5, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Генерация изображений на вход энкодера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def generate_image(img):\n",
    "    \n",
    "    gen_img_par = Variable(img, requires_grad=True)\n",
    "    latent_vector = torch.eye(50)\n",
    "    loss_stats = []\n",
    "    \n",
    "    gen_optim = optim.Adam([gen_img_par], lr=1e-3, weight_decay=0.)\n",
    "\n",
    "    for i in range(1, 500001):  \n",
    "                \n",
    "        target = model.E(gen_img_par.view(-1, 28*28))\n",
    "                \n",
    "        gen_optim.zero_grad()\n",
    "        loss = F.mse_loss(target, latent_vector)    \n",
    "        loss.backward()                   \n",
    "        gen_optim.step()\n",
    "        \n",
    "        if i % 20000 == 0:    \n",
    "            print (\"Iteration {}: loss={}\".format(i,loss))\n",
    "            ls = loss.detach().numpy()            \n",
    "            loss_stats.append(ls)\n",
    "                        \n",
    "            if len(loss_stats) > 5:    \n",
    "                if ls > loss_stats[-2] and ls > loss_stats[-3]:\n",
    "                    print (\"Loss is rising .. stop\")\n",
    "                    break\n",
    "\n",
    "        if i % 100000 == 0:\n",
    "            to_plot = gen_img_par.view(-1, 1, 28, 28).clamp(-1, 1).data.numpy()\n",
    "            plot_mnist(to_plot, (5, 10))\n",
    "                        \n",
    "    return gen_img_par    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img, _ = next(iter(test_loader))\n",
    "to_plot = img.view(-1, 1, 28, 28).clamp(-1, 1).data.numpy()\n",
    "plot_mnist(to_plot, (5, 10))\n",
    "new_image_batch = generate_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _ = next(iter(test_loader))\n",
    "latent = model.E(data.view(-1, 28*28))\n",
    "print (latent.shape)\n",
    "print (latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = latent.view(-1, 1, 5, 10).clamp(-1, 1).data.numpy()\n",
    "plot_mnist(to_plot, (5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
