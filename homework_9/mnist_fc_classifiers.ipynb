{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils import mnist, plot_graphs, plot_mnist, to_onehot\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_tanh = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,)),\n",
    "                lambda x: x.to(device)\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "lr = 0.0001\n",
    "prior_size = 10\n",
    "train_epoch = 200\n",
    "batch_size = 250\n",
    "\n",
    "train_loader, valid_loader, test_loader = mnist(batch_size=batch_size, valid=10000, transform=mnist_tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh(), flatten=False, \n",
    "                 last_fn=None, first_fn=None, device='cpu'):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        self.flatten = flatten\n",
    "        if first_fn is not None:\n",
    "            layers.append(first_fn)\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            layers.append(activation_fn) # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "        else: \n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        if last_fn is not None:\n",
    "            layers.append(last_fn)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.flatten:\n",
    "            x = x.view(x.shape[0], -1)\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self, train_loader, valid_loader, test_loader):\n",
    "        self.net = FullyConnected([28*28, 1024, 1024, prior_size], activation_fn=nn.ReLU(), flatten=True, device=device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.loss = 0.0\n",
    "        self.loss_log = []\n",
    "        self.acc_log = []\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        train_size = len(self.train_loader.sampler)        \n",
    "        for batch_idx, (data, label) in enumerate(self.train_loader):\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # train\n",
    "            self.optimizer.zero_grad()              \n",
    "            output = self.net(data)       \n",
    "            self.loss = self.criterion(output, label)\n",
    "\n",
    "            self.loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                    epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(self.train_loader))\n",
    "                losses = '{:.4f}'.format(self.loss.item())\n",
    "                print(line + losses)\n",
    "\n",
    "        else:\n",
    "            batch_idx += 1\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(self.train_loader))\n",
    "            losses = '{:.4f}'.format(self.loss.item())\n",
    "            print(line + losses)            \n",
    "        \n",
    "    def validation(self, epoch):\n",
    "        val_size = len(self.valid_loader)\n",
    "        loss = 0.\n",
    "        total = 0\n",
    "        correct = 0        \n",
    "        with torch.no_grad():\n",
    "            for data, label in self.valid_loader:\n",
    "                label = label.to(device)\n",
    "                output = self.net(data)\n",
    "                loss += self.criterion(output, label)\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += label.size(0)\n",
    "                correct += (predicted == label).sum().item()\n",
    "\n",
    "            loss /= val_size       \n",
    "            \n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        report = 'Valid loss: {:.4f} Accuracy: {:.2f} \\n'.format(loss, accuracy)\n",
    "        print(report)    \n",
    "        \n",
    "        self.loss_log.append(loss.item())\n",
    "        self.acc_log.append(accuracy)\n",
    "\n",
    "    def start_training(self, train_epoch):\n",
    "        for epoch in range(1, train_epoch + 1):\n",
    "            self.net.train()\n",
    "            self.train(epoch)\n",
    "            self.net.eval()   \n",
    "            self.validation(epoch)\n",
    "            \n",
    "    def test(self):\n",
    "        test_size = len(self.test_loader)\n",
    "        loss = 0.\n",
    "        total = 0\n",
    "        correct = 0        \n",
    "        with torch.no_grad():\n",
    "            for data, label in self.test_loader:\n",
    "                label = label.to(device)\n",
    "                output = self.net(data)\n",
    "                loss += self.criterion(output, label)\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += label.size(0)\n",
    "                correct += (predicted == label).sum().item()\n",
    "\n",
    "            loss /= test_size       \n",
    "            \n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        report = 'Test loss: {:.4f} Accuracy: {:.2f} \\n'.format(loss, accuracy)\n",
    "        print(report)    \n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификатор на исходных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLosses 2.3126\n",
      "Train Epoch: 1 [25000/50000 (50%)]\tLosses 0.4073\n",
      "Train Epoch: 1 [50000/50000 (100%)]\tLosses 0.3814\n",
      "Valid loss: 0.3306 Accuracy: 90.29 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls_raw = Classifier(train_loader, valid_loader, test_loader)\n",
    "cls_raw.start_training(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33062952756881714]\n",
      "[90.29]\n"
     ]
    }
   ],
   "source": [
    "print (cls_raw.loss_log)\n",
    "print (cls_raw.acc_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.3141 Accuracy: 91.06 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls_raw.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификатор на данных CAAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ptype='train', path=''):\n",
    "        self.type = ptype \n",
    "        self.data = torch.randn((50000, 1, 28, 28))\n",
    "        self.labels = torch.ones((50000)).to(torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(str(self.data))\n",
    "\n",
    "    def getData(self):\n",
    "        return self.data\n",
    "    \n",
    "    def sampler(self):\n",
    "        return self.data\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        data, labels  = self.data[index], self.labels[index]        \n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "caae_train = CustomDataset('train', './caae_train_data')\n",
    "caae_valid = CustomDataset('valid', './caae_valid_data')\n",
    "caae_test = CustomDataset('test', './caae_test_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "caae_train_indices = list(range(caae_train.data.shape[0]))\n",
    "caae_valid_indices = list(range(caae_valid.data.shape[0]))\n",
    "caae_test_indices = list(range(caae_test.data.shape[0]))\n",
    "\n",
    "caae_train_loader = DataLoader(caae_train, batch_size=250, sampler=SubsetRandomSampler(caae_train_indices))\n",
    "caae_valid_loader = DataLoader(caae_valid, batch_size=250, sampler=SubsetRandomSampler(caae_valid_indices))\n",
    "caae_test_loader = DataLoader(caae_test, batch_size=250, sampler=SubsetRandomSampler(caae_test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLosses 2.3411\n",
      "Train Epoch: 1 [25000/50000 (50%)]\tLosses 0.0016\n",
      "Train Epoch: 1 [50000/50000 (100%)]\tLosses 0.0007\n",
      "Valid loss: 0.0007 Accuracy: 100.00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls_caae = Classifier(caae_train_loader, caae_valid_loader, caae_test_loader)\n",
    "cls_caae.start_training(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_caae.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
